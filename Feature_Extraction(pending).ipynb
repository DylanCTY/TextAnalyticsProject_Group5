{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaCmbuyvDTLZY298DXDn2j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DylanCTY/TextAnalyticsProject_Group5/blob/main/Feature_Extraction(pending).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================\n",
        "##            VOICEBOOK REPOSITORY            ##      \n",
        "================================================\n",
        "\n",
        "repository name: voicebook\n",
        "repository version: 1.0\n",
        "repository link: https://github.com/jim-schwoebel/voicebook\n",
        "author: Jim Schwoebel\n",
        "author contact: js@neurolex.co\n",
        "description: a book and repo to get you started programming voice applications in Python - 10 chapters and 200+ scripts.\n",
        "license category: opensource\n",
        "license: Apache 2.0 license\n",
        "organization name: NeuroLex Laboratories, Inc.\n",
        "location: Seattle, WA\n",
        "website: https://neurolex.ai\n",
        "release date: 2018-09-28\n",
        "\n",
        "This code (voicebook) is hereby released under a Apache 2.0 license license.\n",
        "\n",
        "For more information, check out the license terms below.\n",
        "\n",
        "================================================\n",
        "##               LICENSE TERMS                ##      \n",
        "================================================\n",
        "\n",
        "Copyright 2018 NeuroLex Laboratories, Inc.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "     http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "================================================\n",
        "##               SERVICE STATEMENT            ##        \n",
        "================================================\n",
        "\n",
        "If you are using the code written for a larger project, we are\n",
        "happy to consult with you and help you with deployment. Our team\n",
        "has >10 world experts in Kafka distributed architectures, microservices\n",
        "built on top of Node.js / Python / Docker, and applying machine learning to\n",
        "model speech and text data.\n",
        "\n",
        "We have helped a wide variety of enterprises - small businesses,\n",
        "researchers, enterprises, and/or independent developers.\n",
        "\n",
        "If you would like to work with us let us know @ js@neurolex.co."
      ],
      "metadata": {
        "id": "PJOALlymcfd7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "================================================\n",
        "##            TRAIN_AUDIOCLASSIFY.PY          ##    \n",
        "================================================"
      ],
      "metadata": {
        "id": "FIhDFRFBctxR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes in two folders with wav files (20 secs),\n",
        "fingerprints them with audio features (.json),\n",
        "and then builds an optimized machine learning model from these features."
      ],
      "metadata": {
        "id": "g7SU3Njdc04q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['mfcc1_mean_(0.02 second window)', 'mfcc1_std_(0.02 second window)', 'mfcc1_max_(0.02 second window)', 'mfcc1_min_(0.02 second window)',\n",
        "\t\t'mfcc2_mean_(0.02 second window)', 'mfcc2_std_(0.02 second window)', 'mfcc2_max_(0.02 second window)', 'mfcc2_min_(0.02 second window)',\n",
        "\t\t'mfcc3_mean_(0.02 second window)', 'mfcc3_std_(0.02 second window)', 'mfcc3_max_(0.02 second window)', 'mfcc3_min_(0.02 second window)',\n",
        "\t\t'mfcc4_mean_(0.02 second window)', 'mfcc4_std_(0.02 second window)', 'mfcc4_max_(0.02 second window)', 'mfcc4_min_(0.02 second window)',\n",
        "\t\t'mfcc5_mean_(0.02 second window)', 'mfcc5_std_(0.02 second window)', 'mfcc5_max_(0.02 second window)', 'mfcc5_min_(0.02 second window)',\n",
        "\t\t'mfcc6_mean_(0.02 second window)', 'mfcc6_std_(0.02 second window)', 'mfcc6_max_(0.02 second window)', 'mfcc6_min_(0.02 second window)',\n",
        "\t\t'mfcc7_mean_(0.02 second window)', 'mfcc7_std_(0.02 second window)', 'mfcc7_max_(0.02 second window)', 'mfcc7_min_(0.02 second window)',\n",
        "\t\t'mfcc8_mean_(0.02 second window)', 'mfcc8_std_(0.02 second window)', 'mfcc8_max_(0.02 second window)', 'mfcc8_min_(0.02 second window)',\n",
        "\t\t'mfcc9_mean_(0.02 second window)', 'mfcc9_std_(0.02 second window)', 'mfcc9_max_(0.02 second window)', 'mfcc9_min_(0.02 second window)',\n",
        "\t\t'mfcc10_mean_(0.02 second window)', 'mfcc10_std_(0.02 second window)', 'mfcc10_max_(0.02 second window)', 'mfcc10_min_(0.02 second window)',\n",
        "\t\t'mfcc11_mean_(0.02 second window)', 'mfcc11_std_(0.02 second window)', 'mfcc11_max_(0.02 second window)', 'mfcc11_min_(0.02 second window)',\n",
        "\t\t'mfcc12_mean_(0.02 second window)', 'mfcc12_std_(0.02 second window)', 'mfcc12_max_(0.02 second window)', 'mfcc12_min_(0.02 second window)',\n",
        "\t\t'mfcc13_mean_(0.02 second window)', 'mfcc13_std_(0.02 second window)', 'mfcc13_max_(0.02 second window)', 'mfcc13_min_(0.02 second window)',\n",
        "\t\t'mfccdelta1_mean_(0.02 second window)', 'mfccdelta1_std_(0.02 second window)', 'mfccdelta1_max_(0.02 second window)', 'mfccdelta1_min_(0.02 second window)',\n",
        "\t\t'mfccdelta2_mean_(0.02 second window)', 'mfccdelta2_std_(0.02 second window)', 'mfccdelta2_max_(0.02 second window)', 'mfccdelta2_min_(0.02 second window)',\n",
        "\t\t'mfccdelta3_mean_(0.02 second window)', 'mfccdelta3_std_(0.02 second window)', 'mfccdelta3_max_(0.02 second window)', 'mfccdelta3_min_(0.02 second window)',\n",
        "\t\t'mfccdelta4_mean_(0.02 second window)', 'mfccdelta4_std_(0.02 second window)', 'mfccdelta4_max_(0.02 second window)', 'mfccdelta4_min_(0.02 second window)',\n",
        "\t\t'mfccdelta5_mean_(0.02 second window)', 'mfccdelta5_std_(0.02 second window)', 'mfccdelta5_max_(0.02 second window)', 'mfccdelta5_min_(0.02 second window)',\n",
        "\t\t'mfccdelta6_mean_(0.02 second window)', 'mfccdelta6_std_(0.02 second window)', 'mfccdelta6_max_(0.02 second window)', 'mfccdelta6_min_(0.02 second window)',\n",
        "\t\t'mfccdelta7_mean_(0.02 second window)', 'mfccdelta7_std_(0.02 second window)', 'mfccdelta7_max_(0.02 second window)', 'mfccdelta7_min_(0.02 second window)',\n",
        "\t\t'mfccdelta8_mean_(0.02 second window)', 'mfccdelta8_std_(0.02 second window)', 'mfccdelta8_max_(0.02 second window)', 'mfccdelta8_min_(0.02 second window)',\n",
        "\t\t'mfccdelta9_mean_(0.02 second window)', 'mfccdelta9_std_(0.02 second window)', 'mfccdelta9_max_(0.02 second window)', 'mfccdelta9_min_(0.02 second window)',\n",
        "\t\t'mfccdelta10_mean_(0.02 second window)', 'mfccdelta10_std_(0.02 second window)', 'mfccdelta10_max_(0.02 second window)', 'mfccdelta10_min_(0.02 second window)',\n",
        "\t\t'mfccdelta11_mean_(0.02 second window)', 'mfccdelta11_std_(0.02 second window)', 'mfccdelta11_max_(0.02 second window)', 'mfccdelta11_min_(0.02 second window)',\n",
        "\t\t'mfccdelta12_mean_(0.02 second window)', 'mfccdelta12_std_(0.02 second window)', 'mfccdelta12_max_(0.02 second window)', 'mfccdelta12_min_(0.02 second window)',\n",
        "\t\t'mfccdelta13_mean_(0.02 second window)', 'mfccdelta13_std_(0.02 second window)', 'mfccdelta13_max_(0.02 second window)', 'mfccdelta13_min_(0.02 second window)',\n",
        "\t\t'mfcc1_mean_(0.50 second window)', 'mfcc1_std_(0.50 second window)', 'mfcc1_max_(0.50 second window)', 'mfcc1_min_(0.50 second window)',\n",
        "\t\t'mfcc2_mean_(0.50 second window)', 'mfcc2_std_(0.50 second window)', 'mfcc2_max_(0.50 second window)', 'mfcc2_min_(0.50 second window)',\n",
        "\t\t'mfcc3_mean_(0.50 second window)', 'mfcc3_std_(0.50 second window)', 'mfcc3_max_(0.50 second window)', 'mfcc3_min_(0.50 second window)',\n",
        "\t\t'mfcc4_mean_(0.50 second window)', 'mfcc4_std_(0.50 second window)', 'mfcc4_max_(0.50 second window)', 'mfcc4_min_(0.50 second window)',\n",
        "\t\t'mfcc5_mean_(0.50 second window)', 'mfcc5_std_(0.50 second window)', 'mfcc5_max_(0.50 second window)', 'mfcc5_min_(0.50 second window)',\n",
        "\t\t'mfcc6_mean_(0.50 second window)', 'mfcc6_std_(0.50 second window)', 'mfcc6_max_(0.50 second window)', 'mfcc6_min_(0.50 second window)',\n",
        "\t\t'mfcc7_mean_(0.50 second window)', 'mfcc7_std_(0.50 second window)', 'mfcc7_max_(0.50 second window)', 'mfcc7_min_(0.50 second window)',\n",
        "\t\t'mfcc8_mean_(0.50 second window)', 'mfcc8_std_(0.50 second window)', 'mfcc8_max_(0.50 second window)', 'mfcc8_min_(0.50 second window)',\n",
        "\t\t'mfcc9_mean_(0.50 second window)', 'mfcc9_std_(0.50 second window)', 'mfcc9_max_(0.50 second window)', 'mfcc9_min_(0.50 second window)',\n",
        "\t\t'mfcc10_mean_(0.50 second window)', 'mfcc10_std_(0.50 second window)', 'mfcc10_max_(0.50 second window)', 'mfcc10_min_(0.50 second window)',\n",
        "\t\t'mfcc11_mean_(0.50 second window)', 'mfcc11_std_(0.50 second window)', 'mfcc11_max_(0.50 second window)', 'mfcc11_min_(0.50 second window)',\n",
        "\t\t'mfcc12_mean_(0.50 second window)', 'mfcc12_std_(0.50 second window)', 'mfcc12_max_(0.50 second window)', 'mfcc12_min_(0.50 second window)',\n",
        "\t\t'mfcc13_mean_(0.50 second window)', 'mfcc13_std_(0.50 second window)', 'mfcc13_max_(0.50 second window)', 'mfcc13_min_(0.50 second window)',\n",
        "\t\t'mfccdelta1_mean_(0.50 second window)', 'mfccdelta1_std_(0.50 second window)', 'mfccdelta1_max_(0.50 second window)', 'mfccdelta1_min_(0.50 second window)',\n",
        "\t\t'mfccdelta2_mean_(0.50 second window)', 'mfccdelta2_std_(0.50 second window)', 'mfccdelta2_max_(0.50 second window)', 'mfccdelta2_min_(0.50 second window)',\n",
        "\t\t'mfccdelta3_mean_(0.50 second window)', 'mfccdelta3_std_(0.50 second window)', 'mfccdelta3_max_(0.50 second window)', 'mfccdelta3_min_(0.50 second window)',\n",
        "\t\t'mfccdelta4_mean_(0.50 second window)', 'mfccdelta4_std_(0.50 second window)', 'mfccdelta4_max_(0.50 second window)', 'mfccdelta4_min_(0.50 second window)',\n",
        "\t\t'mfccdelta5_mean_(0.50 second window)', 'mfccdelta5_std_(0.50 second window)', 'mfccdelta5_max_(0.50 second window)', 'mfccdelta5_min_(0.50 second window)',\n",
        "\t\t'mfccdelta6_mean_(0.50 second window)', 'mfccdelta6_std_(0.50 second window)', 'mfccdelta6_max_(0.50 second window)', 'mfccdelta6_min_(0.50 second window)',\n",
        "\t\t'mfccdelta7_mean_(0.50 second window)', 'mfccdelta7_std_(0.50 second window)', 'mfccdelta7_max_(0.50 second window)', 'mfccdelta7_min_(0.50 second window)',\n",
        "\t\t'mfccdelta8_mean_(0.50 second window)', 'mfccdelta8_std_(0.50 second window)', 'mfccdelta8_max_(0.50 second window)', 'mfccdelta8_min_(0.50 second window)',\n",
        "\t\t'mfccdelta9_mean_(0.50 second window)', 'mfccdelta9_std_(0.50 second window)', 'mfccdelta9_max_(0.50 second window)', 'mfccdelta9_min_(0.50 second window)',\n",
        "\t\t'mfccdelta10_mean_(0.50 second window)', 'mfccdelta10_std_(0.50 second window)', 'mfccdelta10_max_(0.50 second window)', 'mfccdelta10_min_(0.50 second window)',\n",
        "\t\t'mfccdelta11_mean_(0.50 second window)', 'mfccdelta11_std_(0.50 second window)', 'mfccdelta11_max_(0.50 second window)', 'mfccdelta11_min_(0.50 second window)',\n",
        "\t\t'mfccdelta12_mean_(0.50 second window)', 'mfccdelta12_std_(0.50 second window)', 'mfccdelta12_max_(0.50 second window)', 'mfccdelta12_min_(0.50 second window)']\n"
      ],
      "metadata": {
        "id": "d7nqKUPyc5nB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#^ the features above are selected because it preserves some of the heirarchical nature of the data\n",
        "(features over entire length and over time series). Also, there are many research papers that separate out\n",
        "various groups using mfcc coefficients, well-known for dialect and gender detection.\n",
        "\n",
        "#^ note also that for audio files of varying length, we can average out the embeddings over 0.5 second period to\n",
        "time series of N length, making the feature representation simpler.\n",
        "\n",
        "#^ it is this approach that we take in the current representation [208 features]\n",
        "\n",
        "The models tested here include:\n",
        "    -Naive Bayes\n",
        "    -Decision tree\n",
        "    -Support vector machines\n",
        "    -Bernoulli\n",
        "    -Maximum entropy\n",
        "    -Adaboost\n",
        "    -Gradient boost\n",
        "    -Logistic regression\n",
        "    -Hard voting\n",
        "    -K nearest neighbors\n",
        "    -Random forest\n",
        "    -SVM algorithm\n",
        "    -... [future: Deep learning models, etc.]\n",
        "\n",
        "The output is an optimized machine learning model to a feature as a\n",
        ".pickle file, which can be easily imported into the future through code like:\n",
        "\n",
        "    import pickle\n",
        "    f = open(classifiername+'_%s'%(selectedfeature)+'.pickle', 'rb')\n",
        "    classifier = pickle.load(function(f))\n",
        "    ##where function is the feature\n",
        "    f.close()\n",
        "    ##classify with proper function...\n",
        "    classifier.classify(startword(text))\n",
        "\n",
        "Happy modeling!!"
      ],
      "metadata": {
        "id": "2GwYc6khdDg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "!pip install pydub\n",
        "from pydub import AudioSegment\n",
        "import os, nltk, random, json\n",
        "from nltk import word_tokenize\n",
        "from nltk.classify import apply_features, SklearnClassifier, maxent\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "from textblob import TextBlob\n",
        "from operator import itemgetter\n",
        "import getpass\n",
        "import numpy as np\n",
        "import pickle\n",
        "import datetime\n",
        "import time"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zugUXieiD8__",
        "outputId": "dbd34397-3bdf-4b49-c496-f5076c150142"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INITIAL FUNCTIONS\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "4ZumDa0ZF_Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimizemodel_sc(train_set2,labels_train_set2,test_set2,labels_test_set2,modelname,classes,testing_set,min_num,selectedfeature,training_data):\n",
        "    filename=modelname\n",
        "    start=time.time()\n",
        "    jmsgs=train_set2+test_set2\n",
        "    omsgs=labels_train_set2+labels_test_set2\n",
        "\n",
        "    c1=0\n",
        "    c5=0\n",
        "\n",
        "    try:\n",
        "        #decision tree\n",
        "        classifier2 = DecisionTreeClassifier(random_state=0)\n",
        "        classifier2.fit(train_set2,labels_train_set2)\n",
        "        scores = cross_val_score(classifier2, test_set2, labels_test_set2,cv=5)\n",
        "        print('Decision tree accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c2=scores.mean()\n",
        "        c2s=scores.std()\n",
        "        print(c2)\n",
        "    except:\n",
        "        c2=0\n",
        "        c2s=0\n",
        "\n",
        "    try:\n",
        "        classifier3 = GaussianNB()\n",
        "        classifier3.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier3, test_set2, labels_test_set2,cv=5)\n",
        "        print('Gaussian NB accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c3=scores.mean()\n",
        "        c3s=scores.std()\n",
        "        print(c3)\n",
        "    except:\n",
        "        c3=0\n",
        "        c3s=0\n",
        "\n",
        "    try:\n",
        "        #svc\n",
        "        classifier4 = SVC()\n",
        "        classifier4.fit(train_set2,labels_train_set2)\n",
        "        scores=cross_val_score(classifier4, test_set2, labels_test_set2,cv=5)\n",
        "        print('SKlearn classifier accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c4=scores.mean()\n",
        "        c4s=scores.std()\n",
        "        print(c4)\n",
        "    except:\n",
        "        c4=0\n",
        "        c4s=0\n",
        "\n",
        "    try:\n",
        "        #adaboost\n",
        "        classifier6 = AdaBoostClassifier(n_estimators=100)\n",
        "        classifier6.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier6, test_set2, labels_test_set2,cv=5)\n",
        "        print('Adaboost classifier accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c6=scores.mean()\n",
        "        c6s=scores.std()\n",
        "        print(c6)\n",
        "    except:\n",
        "        c6=0\n",
        "        c6s=0\n",
        "\n",
        "    try:\n",
        "        #gradient boosting\n",
        "        classifier7=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "        classifier7.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier7, test_set2, labels_test_set2,cv=5)\n",
        "        print('Gradient boosting accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c7=scores.mean()\n",
        "        c7s=scores.std()\n",
        "        print(c7)\n",
        "    except:\n",
        "        c7=0\n",
        "        c7s=0\n",
        "\n",
        "    try:\n",
        "        #logistic regression\n",
        "        classifier8=LogisticRegression(random_state=1)\n",
        "        classifier8.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier8, test_set2, labels_test_set2,cv=5)\n",
        "        print('Logistic regression accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c8=scores.mean()\n",
        "        c8s=scores.std()\n",
        "        print(c8)\n",
        "    except:\n",
        "        c8=0\n",
        "        c8s=0\n",
        "\n",
        "    try:\n",
        "        #voting\n",
        "        classifier9=VotingClassifier(estimators=[('gradboost', classifier7), ('logit', classifier8), ('adaboost', classifier6)], voting='hard')\n",
        "        classifier9.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier9, test_set2, labels_test_set2,cv=5)\n",
        "        print('Hard voting accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c9=scores.mean()\n",
        "        c9s=scores.std()\n",
        "        print(c9)\n",
        "    except:\n",
        "        c9=0\n",
        "        c9s=0\n",
        "\n",
        "    try:\n",
        "        #knn\n",
        "        classifier10=KNeighborsClassifier(n_neighbors=7)\n",
        "        classifier10.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier10, test_set2, labels_test_set2,cv=5)\n",
        "        print('K Nearest Neighbors accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c10=scores.mean()\n",
        "        c10s=scores.std()\n",
        "        print(c10)\n",
        "    except:\n",
        "        c10=0\n",
        "        c10s=0\n",
        "\n",
        "    try:\n",
        "        #randomforest\n",
        "        classifier11=RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
        "        classifier11.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier11, test_set2, labels_test_set2,cv=5)\n",
        "        print('Random forest accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c11=scores.mean()\n",
        "        c11s=scores.std()\n",
        "        print(c11)\n",
        "    except:\n",
        "        c11=0\n",
        "        c11s=0\n",
        "\n",
        "    try:\n",
        "##        #svm\n",
        "        classifier12 = svm.SVC(kernel='linear', C = 1.0)\n",
        "        classifier12.fit(train_set2, labels_train_set2)\n",
        "        scores = cross_val_score(classifier12, test_set2, labels_test_set2,cv=5)\n",
        "        print('svm accuracy (+/-) %s'%(str(scores.std())))\n",
        "        c12=scores.mean()\n",
        "        c12s=scores.std()\n",
        "        print(c12)\n",
        "    except:\n",
        "        c12=0\n",
        "        c12s=0\n",
        "\n",
        "    #IF IMBALANCED, USE http://scikit-learn.org/dev/modules/generated/sklearn.naive_bayes.ComplementNB.html\n",
        "\n",
        "    maxacc=max([c2,c3,c4,c6,c7,c8,c9,c10,c11,c12])\n",
        "\n",
        "    if maxacc==c1:\n",
        "        print('most accurate classifier is Naive Bayes'+'with %s'%(selectedfeature))\n",
        "        classifiername='naive-bayes'\n",
        "        classifier=classifier1\n",
        "        #show most important features\n",
        "        classifier1.show_most_informative_features(5)\n",
        "    elif maxacc==c2:\n",
        "        print('most accurate classifier is Decision Tree'+'with %s'%(selectedfeature))\n",
        "        classifiername='decision-tree'\n",
        "        classifier2 = DecisionTreeClassifier(random_state=0)\n",
        "        classifier2.fit(train_set2+test_set2,labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier2\n",
        "    elif maxacc==c3:\n",
        "        print('most accurate classifier is Gaussian NB'+'with %s'%(selectedfeature))\n",
        "        classifiername='gaussian-nb'\n",
        "        classifier3 = GaussianNB()\n",
        "        classifier3.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier3\n",
        "    elif maxacc==c4:\n",
        "        print('most accurate classifier is SK Learn'+'with %s'%(selectedfeature))\n",
        "        classifiername='sk'\n",
        "        classifier4 = SVC()\n",
        "        classifier4.fit(train_set2+test_set2,labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier4\n",
        "    elif maxacc==c5:\n",
        "        print('most accurate classifier is Maximum Entropy Classifier'+'with %s'%(selectedfeature))\n",
        "        classifiername='max-entropy'\n",
        "        classifier=classifier5\n",
        "    #can stop here (c6-c10)\n",
        "    elif maxacc==c6:\n",
        "        print('most accuracate classifier is Adaboost classifier'+'with %s'%(selectedfeature))\n",
        "        classifiername='adaboost'\n",
        "        classifier6 = AdaBoostClassifier(n_estimators=100)\n",
        "        classifier6.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier6\n",
        "    elif maxacc==c7:\n",
        "        print('most accurate classifier is Gradient Boosting '+'with %s'%(selectedfeature))\n",
        "        classifiername='graidentboost'\n",
        "        classifier7=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "        classifier7.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier7\n",
        "    elif maxacc==c8:\n",
        "        print('most accurate classifier is Logistic Regression '+'with %s'%(selectedfeature))\n",
        "        classifiername='logistic_regression'\n",
        "        classifier8=LogisticRegression(random_state=1)\n",
        "        classifier8.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier8\n",
        "    elif maxacc==c9:\n",
        "        print('most accurate classifier is Hard Voting '+'with %s'%(selectedfeature))\n",
        "        classifiername='hardvoting'\n",
        "        classifier7=GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
        "        classifier7.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier8=LogisticRegression(random_state=1)\n",
        "        classifier8.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier6 = AdaBoostClassifier(n_estimators=100)\n",
        "        classifier6.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier9=VotingClassifier(estimators=[('gradboost', classifier7), ('logit', classifier8), ('adaboost', classifier6)], voting='hard')\n",
        "        classifier9.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier9\n",
        "    elif maxacc==c10:\n",
        "        print('most accurate classifier is K nearest neighbors '+'with %s'%(selectedfeature))\n",
        "        classifiername='knn'\n",
        "        classifier10=KNeighborsClassifier(n_neighbors=7)\n",
        "        classifier10.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier10\n",
        "    elif maxacc==c11:\n",
        "        print('most accurate classifier is Random forest '+'with %s'%(selectedfeature))\n",
        "        classifiername='randomforest'\n",
        "        classifier11=RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
        "        classifier11.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier11\n",
        "    elif maxacc==c12:\n",
        "        print('most accurate classifier is SVM '+' with %s'%(selectedfeature))\n",
        "        classifiername='svm'\n",
        "        classifier12 = svm.SVC(kernel='linear', C = 1.0)\n",
        "        classifier12.fit(train_set2+test_set2, labels_train_set2+labels_test_set2)\n",
        "        classifier=classifier12\n",
        "\n",
        "    modeltypes=['decision-tree','gaussian-nb','sk','adaboost','gradient boosting','logistic regression','hard voting','knn','random forest','svm']\n",
        "    accuracym=[c2,c3,c4,c6,c7,c8,c9,c10,c11,c12]\n",
        "    accuracys=[c2s,c3s,c4s,c6s,c7s,c8s,c9s,c10s,c11s,c12s]\n",
        "    model_accuracy=list()\n",
        "    for i in range(len(modeltypes)):\n",
        "        model_accuracy.append([modeltypes[i],accuracym[i],accuracys[i]])\n",
        "\n",
        "    model_accuracy.sort(key=itemgetter(1))\n",
        "    endlen=len(model_accuracy)\n",
        "\n",
        "    print('saving classifier to disk')\n",
        "    f=open(modelname+'.pickle','wb')\n",
        "    pickle.dump(classifier,f)\n",
        "    f.close()\n",
        "\n",
        "    end=time.time()\n",
        "\n",
        "    execution=end-start\n",
        "\n",
        "    print('summarizing session...')\n",
        "\n",
        "    accstring=''\n",
        "\n",
        "    for i in range(len(model_accuracy)):\n",
        "        accstring=accstring+'%s: %s (+/- %s)\\n'%(str(model_accuracy[i][0]),str(model_accuracy[i][1]),str(model_accuracy[i][2]))\n",
        "\n",
        "    training=len(train_set2)\n",
        "    testing=len(test_set2)\n",
        "\n",
        "    summary='SUMMARY OF MODEL SELECTION \\n\\n'+'WINNING MODEL: \\n\\n'+'%s: %s (+/- %s) \\n\\n'%(str(model_accuracy[len(model_accuracy)-1][0]),str(model_accuracy[len(model_accuracy)-1][1]),str(model_accuracy[len(model_accuracy)-1][2]))+'MODEL FILE NAME: \\n\\n %s.pickle'%(filename)+'\\n\\n'+'DATE CREATED: \\n\\n %s'%(datetime.datetime.now())+'\\n\\n'+'EXECUTION TIME: \\n\\n %s\\n\\n'%(str(execution))+'GROUPS: \\n\\n'+str(classes)+'\\n'+'('+str(min_num)+' in each class, '+str(int(testing_set*100))+'% used for testing)'+'\\n\\n'+'TRAINING SUMMARY:'+'\\n\\n'+training_data+'FEATURES: \\n\\n %s'%(selectedfeature)+'\\n\\n'+'MODELS, ACCURACIES, AND STANDARD DEVIATIONS: \\n\\n'+accstring+'\\n\\n'+'(C) 2018, NeuroLex Laboratories'\n",
        "\n",
        "    data={\n",
        "        'model':modelname,\n",
        "        'modeltype':model_accuracy[len(model_accuracy)-1][0],\n",
        "        'accuracy':model_accuracy[len(model_accuracy)-1][1],\n",
        "        'deviation':model_accuracy[len(model_accuracy)-1][2]\n",
        "        }\n",
        "\n",
        "    return [classifier, model_accuracy[endlen-1], summary, data]\n",
        "\n",
        "def featurize(wavfile):\n",
        "    #initialize features\n",
        "    hop_length = 512\n",
        "    n_fft=2048\n",
        "    #load file\n",
        "    y, sr = librosa.load(wavfile)\n",
        "    #extract mfcc coefficients\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, hop_length=hop_length, n_mfcc=13)\n",
        "    mfcc_delta = librosa.feature.delta(mfcc)\n",
        "    #extract mean, standard deviation, min, and max value in mfcc frame, do this across all mfccs\n",
        "    mfcc_features=np.array([np.mean(mfcc[0]),np.std(mfcc[0]),np.amin(mfcc[0]),np.amax(mfcc[0]),\n",
        "                            np.mean(mfcc[1]),np.std(mfcc[1]),np.amin(mfcc[1]),np.amax(mfcc[1]),\n",
        "                            np.mean(mfcc[2]),np.std(mfcc[2]),np.amin(mfcc[2]),np.amax(mfcc[2]),\n",
        "                            np.mean(mfcc[3]),np.std(mfcc[3]),np.amin(mfcc[3]),np.amax(mfcc[3]),\n",
        "                            np.mean(mfcc[4]),np.std(mfcc[4]),np.amin(mfcc[4]),np.amax(mfcc[4]),\n",
        "                            np.mean(mfcc[5]),np.std(mfcc[5]),np.amin(mfcc[5]),np.amax(mfcc[5]),\n",
        "                            np.mean(mfcc[6]),np.std(mfcc[6]),np.amin(mfcc[6]),np.amax(mfcc[6]),\n",
        "                            np.mean(mfcc[7]),np.std(mfcc[7]),np.amin(mfcc[7]),np.amax(mfcc[7]),\n",
        "                            np.mean(mfcc[8]),np.std(mfcc[8]),np.amin(mfcc[8]),np.amax(mfcc[8]),\n",
        "                            np.mean(mfcc[9]),np.std(mfcc[9]),np.amin(mfcc[9]),np.amax(mfcc[9]),\n",
        "                            np.mean(mfcc[10]),np.std(mfcc[10]),np.amin(mfcc[10]),np.amax(mfcc[10]),\n",
        "                            np.mean(mfcc[11]),np.std(mfcc[11]),np.amin(mfcc[11]),np.amax(mfcc[11]),\n",
        "                            np.mean(mfcc[12]),np.std(mfcc[12]),np.amin(mfcc[12]),np.amax(mfcc[12]),\n",
        "                            np.mean(mfcc_delta[0]),np.std(mfcc_delta[0]),np.amin(mfcc_delta[0]),np.amax(mfcc_delta[0]),\n",
        "                            np.mean(mfcc_delta[1]),np.std(mfcc_delta[1]),np.amin(mfcc_delta[1]),np.amax(mfcc_delta[1]),\n",
        "                            np.mean(mfcc_delta[2]),np.std(mfcc_delta[2]),np.amin(mfcc_delta[2]),np.amax(mfcc_delta[2]),\n",
        "                            np.mean(mfcc_delta[3]),np.std(mfcc_delta[3]),np.amin(mfcc_delta[3]),np.amax(mfcc_delta[3]),\n",
        "                            np.mean(mfcc_delta[4]),np.std(mfcc_delta[4]),np.amin(mfcc_delta[4]),np.amax(mfcc_delta[4]),\n",
        "                            np.mean(mfcc_delta[5]),np.std(mfcc_delta[5]),np.amin(mfcc_delta[5]),np.amax(mfcc_delta[5]),\n",
        "                            np.mean(mfcc_delta[6]),np.std(mfcc_delta[6]),np.amin(mfcc_delta[6]),np.amax(mfcc_delta[6]),\n",
        "                            np.mean(mfcc_delta[7]),np.std(mfcc_delta[7]),np.amin(mfcc_delta[7]),np.amax(mfcc_delta[7]),\n",
        "                            np.mean(mfcc_delta[8]),np.std(mfcc_delta[8]),np.amin(mfcc_delta[8]),np.amax(mfcc_delta[8]),\n",
        "                            np.mean(mfcc_delta[9]),np.std(mfcc_delta[9]),np.amin(mfcc_delta[9]),np.amax(mfcc_delta[9]),\n",
        "                            np.mean(mfcc_delta[10]),np.std(mfcc_delta[10]),np.amin(mfcc_delta[10]),np.amax(mfcc_delta[10]),\n",
        "                            np.mean(mfcc_delta[11]),np.std(mfcc_delta[11]),np.amin(mfcc_delta[11]),np.amax(mfcc_delta[11]),\n",
        "                            np.mean(mfcc_delta[12]),np.std(mfcc_delta[12]),np.amin(mfcc_delta[12]),np.amax(mfcc_delta[12])])\n",
        "\n",
        "    return mfcc_features\n",
        "\n",
        "def exportfile(newAudio,time1,time2,filename,i):\n",
        "    #Exports to a wav file in the current path.\n",
        "    newAudio2 = newAudio[time1:time2]\n",
        "    g=os.listdir()\n",
        "    if filename[0:-4]+'_'+str(i)+'.wav' in g:\n",
        "        filename2=str(i)+'_segment'+'.wav'\n",
        "        print('making %s'%(filename2))\n",
        "        newAudio2.export(filename2,format=\"wav\")\n",
        "    else:\n",
        "        filename2=str(i)+'.wav'\n",
        "        print('making %s'%(filename2))\n",
        "        newAudio2.export(filename2, format=\"wav\")\n",
        "\n",
        "    return filename2\n",
        "\n",
        "def audio_time_features(filename):\n",
        "    #recommend >0.50 seconds for timesplit\n",
        "    timesplit=0.50\n",
        "    hop_length = 512\n",
        "    n_fft=2048\n",
        "\n",
        "    y, sr = librosa.load(filename)\n",
        "    duration=float(librosa.core.get_duration(y))\n",
        "\n",
        "    #Now splice an audio signal into individual elements of 100 ms and extract\n",
        "    #all these features per 100 ms\n",
        "    segnum=round(duration/timesplit)\n",
        "    deltat=duration/segnum\n",
        "    timesegment=list()\n",
        "    time=0\n",
        "\n",
        "    for i in range(segnum):\n",
        "        #milliseconds\n",
        "        timesegment.append(time)\n",
        "        time=time+deltat*1000\n",
        "\n",
        "    newAudio = AudioSegment.from_wav(filename)\n",
        "    filelist=list()\n",
        "\n",
        "    for i in range(len(timesegment)-1):\n",
        "        filename=exportfile(newAudio,timesegment[i],timesegment[i+1],filename,i)\n",
        "        filelist.append(filename)\n",
        "\n",
        "        featureslist=np.array([0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0,\n",
        "                               0,0,0,0])\n",
        "\n",
        "        #save 100 ms segments in current folder (delete them after)\n",
        "        for j in range(len(filelist)):\n",
        "            try:\n",
        "                features=featurize(filelist[i])\n",
        "                featureslist=featureslist+features\n",
        "                os.remove(filelist[j])\n",
        "            except:\n",
        "                print('error splicing')\n",
        "                featureslist.append('silence')\n",
        "                os.remove(filelist[j])\n",
        "\n",
        "        #now scale the featureslist array by the length to get mean in each category\n",
        "        featureslist=featureslist/segnum\n",
        "\n",
        "        return featureslist"
      ],
      "metadata": {
        "id": "9UdS84KbGBrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FEATURIZE .WAV FILES WITH AUDIO FEATURES --> MAKE JSON (if needed)\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "xJaPR3wNGI8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classnum=input('how many classes are you training?')\n",
        "\n",
        "folderlist=list()\n",
        "a=0\n",
        "while a != int(classnum):\n",
        "    folderlist.append(input('what is the folder name for class %s?'%(str(a+1))))\n",
        "    a=a+1\n",
        "\n",
        "name=''\n",
        "for i in range(len(folderlist)):\n",
        "    if i==0:\n",
        "        name=name+folderlist[i]\n",
        "    else:\n",
        "        name=name+'_'+folderlist[i]\n",
        "\n",
        "start=time.time()\n",
        "#modelname=input('what is the name of your classifier?')\n",
        "modelname=name+'_sc_audio'\n",
        "jsonfilename=name+'_audio.json'\n",
        "dir3=os.getcwd()+'/data/'\n",
        "model_dir=os.getcwd()+'/models'\n",
        "cur_dir=dir3\n",
        "testing_set=0.33\n",
        "\n",
        "try:\n",
        "    os.chdir(dir3)\n",
        "except:\n",
        "    os.mkdir(dir3)\n",
        "    os.chdir(dir3)\n",
        "\n",
        "if jsonfilename not in os.listdir():\n",
        "\n",
        "    features_list=list()\n",
        "\n",
        "    for i in range(len(folderlist)):\n",
        "\n",
        "        name=folderlist[i]\n",
        "\n",
        "        dir_=cur_dir+name\n",
        "\n",
        "        g='error'\n",
        "        while g == 'error':\n",
        "            try:\n",
        "                g='noterror'\n",
        "                os.chdir(dir_)\n",
        "            except:\n",
        "                g='error'\n",
        "                print('directory not recognized')\n",
        "                dir_=input('input directory %s path'%(str(i+1)))\n",
        "\n",
        "\n",
        "        #now go through each directory and featurize the samples and save them as .json files\n",
        "        try:\n",
        "            os.chdir(dir_)\n",
        "        except:\n",
        "            os.mkdir(dir_)\n",
        "            os.chdir(dir_)\n",
        "\n",
        "        dirlist=os.listdir()\n",
        "        # remove any prior features\n",
        "        for j in range(len(dirlist)):\n",
        "            if dirlist[j][-5:]=='.json':\n",
        "                os.remove(dirlist[j])\n",
        "\n",
        "        dirlist=os.listdir()\n",
        "        #if broken session, load all previous transcripts\n",
        "        #this reduces costs if tied to GCP\n",
        "        one=list()\n",
        "        for j in range(len(dirlist)):\n",
        "            try:\n",
        "                if dirlist[j][-5:]=='.json':\n",
        "                    #this assumes all .json in the folder are transcript (safe assumption if only .wav files)\n",
        "                    jsonfile=dirlist[j]\n",
        "                    features=json.load(open(jsonfile))['features']\n",
        "                    one.append(features)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for j in range(len(dirlist)):\n",
        "            try:\n",
        "                file = dirlist[j]\n",
        "                if file[-4:]=='.m4a':\n",
        "                    os.system('ffmpeg -i %s %s'%(file, file[0:-4]+'.wav'))\n",
        "                    os.remove(file)\n",
        "                    file=dirlist[j][0:-4]+'.wav'\n",
        "\n",
        "                if file[-4:]=='.wav' not in dirlist and os.path.getsize(file)>500:\n",
        "                    try:\n",
        "                        #get wavefile\n",
        "                        wavfile=file\n",
        "                        print('%s - featurizing %s'%(name.upper(),wavfile))\n",
        "                        #obtain features\n",
        "                        features=np.append(featurize(wavfile),audio_time_features(wavfile))\n",
        "                        print(features)\n",
        "                        #append to list\n",
        "                        one.append(features.tolist())\n",
        "                        #save intermediate .json just in case\n",
        "                        data={\n",
        "                            'features':features.tolist(),\n",
        "                            }\n",
        "                        jsonfile=open(dirlist[j][0:-4]+'.json','w')\n",
        "                        json.dump(data,jsonfile)\n",
        "                        jsonfile.close()\n",
        "                    except:\n",
        "                        print('error')\n",
        "                else:\n",
        "                    pass\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        features_list.append(one)\n",
        "\n",
        "    # randomly shuffle lists\n",
        "    feature_list2=list()\n",
        "    feature_lengths=list()\n",
        "    for i in range(len(features_list)):\n",
        "        one=features_list[i]\n",
        "        random.shuffle(one)\n",
        "        feature_list2.append(one)\n",
        "        feature_lengths.append(len(one))\n",
        "\n",
        "    # remember folderlist has all the labels\n",
        "\n",
        "    min_num=np.amin(feature_lengths)\n",
        "    #make sure they are the same length (For later) - this avoid errors\n",
        "    while min_num*len(folderlist) != np.sum(feature_lengths):\n",
        "        for i in range(len(folderlist)):\n",
        "            while len(feature_list2[i])>min_num:\n",
        "                print('%s is %s more than %s, balancing...'%(folderlist[i].upper(),str(len(feature_list2[i])-int(min_num)),'min value'))\n",
        "                feature_list2[i].pop()\n",
        "        feature_lengths=list()\n",
        "        for i in range(len(feature_list2)):\n",
        "            one=feature_list2[i]\n",
        "            feature_lengths.append(len(one))\n",
        "\n",
        "    #now write to json\n",
        "    data={}\n",
        "    for i in range(len(folderlist)):\n",
        "        data.update({folderlist[i]:feature_list2[i]})\n",
        "\n",
        "    os.chdir(dir3)\n",
        "\n",
        "    jsonfile=open(jsonfilename,'w')\n",
        "    json.dump(data,jsonfile)\n",
        "    jsonfile.close()\n",
        "\n",
        "else:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "DLB7DqpqGON8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREPROCESSING\n",
        "#############################################################"
      ],
      "metadata": {
        "id": "jf2YD-2FGOoU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYUF3xZDcXs-"
      },
      "outputs": [],
      "source": [
        "# note that this assumes a classification problem based on total number of classes\n",
        "os.chdir(cur_dir)\n",
        "\n",
        "#load data - can do this through loading .txt or .json files\n",
        "#json file must have 'message' field\n",
        "data=json.loads(open(jsonfilename).read())\n",
        "\n",
        "classes=list(data)\n",
        "features=list()\n",
        "labels=list()\n",
        "for i in range(len(classes)):\n",
        "    for j in range(len(data[classes[i]])):\n",
        "        feature=data[classes[i]][j]\n",
        "        features.append(feature)\n",
        "        labels.append(classes[i])\n",
        "\n",
        "train_set, test_set, train_labels, test_labels = train_test_split(features,\n",
        "                                                                  labels,\n",
        "                                                                  test_size=testing_set,\n",
        "                                                                  random_state=42)\n",
        "\n",
        "try:\n",
        "    os.chdir(model_dir)\n",
        "except:\n",
        "    os.mkdir(model_dir)\n",
        "    os.chdir(model_dir)\n",
        "\n",
        "g=open(modelname+'_training_data.txt','w')\n",
        "g.write('train labels'+'\\n\\n'+str(train_labels)+'\\n\\n')\n",
        "g.write('test labels'+'\\n\\n'+str(test_labels)+'\\n\\n')\n",
        "g.close()\n",
        "\n",
        "training_data=open(modelname+'_training_data.txt').read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODEL OPTIMIZATION / SAVE TO DISK\n",
        "#################################################################"
      ],
      "metadata": {
        "id": "I3-_2HRgGXln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selectedfeature='audio features (mfcc coefficients).'\n",
        "min_num=len(data[classes[0]])\n",
        "[audio_model, audio_acc, audio_summary, data]=optimizemodel_sc(train_set,train_labels,test_set,test_labels,modelname,classes,testing_set,min_num,selectedfeature,training_data)\n",
        "\n",
        "g=open(modelname+'.txt','w')\n",
        "g.write(audio_summary)\n",
        "g.close()\n",
        "\n",
        "g2=open(modelname+'.json','w')\n",
        "json.dump(data,g2)\n",
        "g2.close()\n",
        "\n",
        "print(audio_model)\n",
        "print(audio_acc)"
      ],
      "metadata": {
        "id": "KrpLsDR_GeIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}